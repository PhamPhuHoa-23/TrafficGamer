# %% [markdown]
# # TrafficGamer (QCNet) + Argoverse 2 - Kaggle Notebook
# 
# Notebook nÃ y load QCNet pretrained vÃ  evaluate trÃªn Argoverse 2 dataset.
# 
# **Repo:** https://github.com/PhamPhuHoa-23/TrafficGamer (Modified QCNet cho Kaggle)
# 
# **Dataset cáº§n add trong Kaggle:**
# - `nek-chua` hoáº·c dataset chá»©a Argoverse 2 train/val data
# - `qcnetckptargoverse` - checkpoint QCNet AV2

# %% [markdown]
# ## 1. Install Dependencies

# %%
# Install cÃ¡c packages cáº§n thiáº¿t
!pip install -q torch torchvision torchaudio
!pip install -q pytorch-lightning==2.0.0
!pip install -q torch-geometric
!pip install -q av2  # Argoverse 2 API
!pip install -q pyarrow  # Äá»ƒ Ä‘á»c parquet files

# %%
# Install PyG dependencies (quan trá»ng cho QCNet)
import torch
print(f"PyTorch version: {torch.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")

# Láº¥y CUDA version Ä‘á»ƒ install Ä‘Ãºng packages
cuda_version = torch.version.cuda.replace('.', '') if torch.cuda.is_available() else 'cpu'
print(f"CUDA version: {cuda_version}")

# %%
# Install torch-scatter, torch-sparse, torch-cluster
!pip install torch-scatter torch-sparse torch-cluster -f https://data.pyg.org/whl/torch-{torch.__version__.split('+')[0]}+cu{cuda_version[:3]}.html

# %% [markdown]
# ## 2. Import Libraries

# %%
import torch
import torch.nn as nn
import pytorch_lightning as pl
from pathlib import Path
import numpy as np
import pandas as pd
from tqdm.auto import tqdm
import pickle
import warnings
warnings.filterwarnings('ignore')

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"âœ… Using device: {device}")

# %% [markdown]
# ## 3. Clone TrafficGamer Repository (Modified QCNet)

# %%
import os

# Clone TrafficGamer repo (QCNet Ä‘Ã£ Ä‘Æ°á»£c sá»­a cho Argoverse 2 trÃªn Kaggle)
if not os.path.exists('TrafficGamer'):
    !git clone https://github.com/PhamPhuHoa-23/TrafficGamer.git
    print("âœ… TrafficGamer cloned")
else:
    print("âœ… TrafficGamer already exists")

# Change directory to TrafficGamer
os.chdir('TrafficGamer')
print(f"ğŸ“ Current directory: {os.getcwd()}")

# %%
# ThÃªm TrafficGamer vÃ o path
import sys
sys.path.insert(0, os.getcwd())

# %% [markdown]
# ## 4. Setup Data Paths
# 
# **Thay Ä‘á»•i paths nÃ y theo dataset cá»§a báº¡n trÃªn Kaggle**

# %%
# ============================================
# ğŸ”§ THAY Äá»”I PATHS á» ÄÃ‚Y THEO KAGGLE DATASET
# ============================================

# Path Ä‘áº¿n Argoverse 2 data
AV2_ROOT = "/kaggle/input/nek-chua"  # Thay Ä‘á»•i theo dataset cá»§a báº¡n

# Path Ä‘áº¿n checkpoint
CKPT_PATH = "/kaggle/input/qcnetckptargoverse/pytorch/default/1/QCNet_AV2.ckpt"

# Hoáº·c download tá»« HuggingFace
# !wget -q https://huggingface.co/ZikangZhou/QCNet/resolve/main/QCNet_AV2.ckpt -O qcnet_av2.ckpt
# CKPT_PATH = "qcnet_av2.ckpt"

print(f"ğŸ“‚ AV2 Root: {AV2_ROOT}")
print(f"ğŸ“¦ Checkpoint: {CKPT_PATH}")

# %% [markdown]
# ## 5. Load vÃ  Cache Scenarios
# 
# **LÆ°u Ã½:** Glob ráº¥t lÃ¢u (~vÃ i phÃºt), nÃªn save vÃ o pkl Ä‘á»ƒ dÃ¹ng láº¡i.
# - Náº¿u Ä‘Ã£ cÃ³ file pkl glob sáºµn â†’ load tá»« pkl (nhanh)
# - Náº¿u chÆ°a cÃ³ â†’ glob vÃ  save pkl

# %%
import pickle

# ============================================
# ğŸ”§ PATHS CHO CACHED SCENARIOS
# ============================================
# Path Ä‘áº¿n file pkl Ä‘Ã£ glob sáºµn (náº¿u cÃ³ upload lÃªn Kaggle dataset)
CACHED_SCENARIOS_INPUT = "/kaggle/input/argoverse-glob/av2_scenarios.pkl"

# Path Ä‘á»ƒ save pkl má»›i (náº¿u cáº§n glob)
CACHED_SCENARIOS_OUTPUT = "av2_scenarios.pkl"

# %%
def load_or_glob_scenarios(cached_input, cached_output, data_root, split='train'):
    """
    Load scenarios tá»« cached pkl hoáº·c glob má»›i náº¿u chÆ°a cÃ³.
    
    Priority:
    1. Load tá»« cached input (uploaded dataset)
    2. Load tá»« cached output (Ä‘Ã£ glob trÆ°á»›c Ä‘Ã³ trong session)
    3. Glob má»›i vÃ  save
    """
    # 1. Try load tá»« input dataset (Ä‘Ã£ upload sáºµn)
    if Path(cached_input).exists():
        print(f"ğŸ“¦ Loading cached scenarios from: {cached_input}")
        with open(cached_input, 'rb') as f:
            scenarios = pickle.load(f)
        print(f"âœ… Loaded {len(scenarios)} scenarios (cached)")
        return scenarios
    
    # 2. Try load tá»« output (Ä‘Ã£ glob trong session nÃ y)
    if Path(cached_output).exists():
        print(f"ğŸ“¦ Loading scenarios from: {cached_output}")
        with open(cached_output, 'rb') as f:
            scenarios = pickle.load(f)
        print(f"âœ… Loaded {len(scenarios)} scenarios")
        return scenarios
    
    # 3. Glob má»›i (cháº­m)
    print("ğŸ” Globbing scenarios... (this may take a few minutes)")
    root = Path(data_root)
    
    # Thá»­ nhiá»u patterns (Kaggle thÆ°á»ng cÃ³ nested structure)
    patterns = [
        f"{split}/{split}/*/scenario_*.parquet",   # Kaggle: train/train/xxx/ â† Try this first!
        f"{split}/*/scenario_*.parquet",            # Standard: train/xxx/
        f"**/scenario_*.parquet",                   # Recursive search (fallback)
    ]
    
    scenarios = []
    for pattern in patterns:
        print(f"  Trying pattern: {pattern}")
        found = list(root.glob(pattern))
        if found:
            scenarios = found
            print(f"  âœ… Found {len(scenarios)} scenarios")
            break
    
    if not scenarios:
        print("âš ï¸ No scenarios found! Listing directory structure:")
        for item in list(root.iterdir())[:5]:
            print(f"  - {item}")
            if item.is_dir():
                for subitem in list(item.iterdir())[:3]:
                    print(f"      - {subitem}")
        return []
    
    # Save Ä‘á»ƒ dÃ¹ng láº¡i
    print(f"ğŸ’¾ Saving to {cached_output}...")
    with open(cached_output, 'wb') as f:
        pickle.dump(scenarios, f)
    print(f"âœ… Saved {len(scenarios)} scenarios")
    
    return scenarios

# %%
# Load scenarios
scenarios = load_or_glob_scenarios(
    cached_input=CACHED_SCENARIOS_INPUT,
    cached_output=CACHED_SCENARIOS_OUTPUT,
    data_root=AV2_ROOT,
    split='train'
)

print(f"\nğŸ“Š Total scenarios: {len(scenarios)}")
if scenarios:
    print(f"ğŸ“„ Example: {scenarios[0]}")

# %% [markdown]
# ## 6. Hiá»ƒu Argoverse 2 Data Format
# 
# Argoverse 2 parquet file chá»©a cÃ¡c columns:
# - `track_id`: ID cá»§a má»—i agent
# - `object_type`: vehicle, pedestrian, cyclist, etc.
# - `object_category`: FOCAL_TRACK, SCORED_TRACK, UNSCORED_TRACK
# - `timestep`: 0-109 (110 timesteps @ 10Hz = 11 seconds)
# - `position_x`, `position_y`: Tá»a Ä‘á»™
# - `heading`: HÆ°á»›ng (radians)
# - `velocity_x`, `velocity_y`: Váº­n tá»‘c

# %%
def load_scenario(parquet_path):
    """Load má»™t scenario tá»« parquet file."""
    df = pd.read_parquet(parquet_path)
    return df

# %%
# Load vÃ  xem má»™t scenario máº«u
if scenarios:
    sample_df = load_scenario(scenarios[0])
    print("ğŸ“‹ Columns:")
    print(sample_df.columns.tolist())
    print("\nğŸ“Š Shape:", sample_df.shape)
    print("\nğŸ” Sample data:")
    print(sample_df.head(10))
    print("\nğŸ“ˆ Object categories:")
    print(sample_df['object_category'].value_counts())
    print("\nğŸš— Object types:")
    print(sample_df['object_type'].value_counts())

# %% [markdown]
# ## 7. Process Argoverse 2 Data cho QCNet

# %%
def process_av2_scenario(df):
    """
    Extract features tá»« Argoverse 2 parquet cho QCNet inference.
    
    Argoverse 2 timeline:
    - Timesteps 0-49: History (5 seconds @ 10Hz)
    - Timesteps 50-109: Future to predict (6 seconds @ 10Hz)
    
    Returns:
        agent_hist: (50, 5) - [x, y, vx, vy, heading]
        gt_future: (60, 2) - [x, y]
        all_agents_hist: Optional - history of all agents
    """
    # ===== 1. TÃ¬m focal agent =====
    # Convert object_category to string to handle different types
    obj_cat = df['object_category'].astype(str).str.upper()
    
    # Try different variations
    focal_mask = obj_cat == 'FOCAL_TRACK'
    if not focal_mask.any():
        focal_mask = obj_cat == 'SCORED_TRACK'
    if not focal_mask.any():
        focal_mask = obj_cat == 'TRACK_FRAGMENT'
    if not focal_mask.any():
        # Fallback: dÃ¹ng track cÃ³ nhiá»u timesteps nháº¥t
        track_counts = df.groupby('track_id').size()
        main_track = track_counts.idxmax()
        focal_mask = df['track_id'] == main_track
        print(f"âš ï¸ No focal track found, using main track: {main_track}")
    
    focal_df = df[focal_mask].sort_values('timestep')
    
    if len(focal_df) == 0:
        raise ValueError("No focal agent found in scenario")
    
    # ===== 2. Extract history (timesteps 0-49) =====
    hist_df = focal_df[focal_df['timestep'] < 50].copy()
    
    # Xá»­ lÃ½ missing timesteps
    required_cols = ['position_x', 'position_y', 'velocity_x', 'velocity_y', 'heading']
    
    # Check vÃ  handle missing columns
    for col in required_cols:
        if col not in hist_df.columns:
            if col == 'velocity_x':
                hist_df['velocity_x'] = hist_df['position_x'].diff() * 10  # 10Hz
            elif col == 'velocity_y':
                hist_df['velocity_y'] = hist_df['position_y'].diff() * 10
            elif col == 'heading':
                hist_df['heading'] = np.arctan2(
                    hist_df['velocity_y'].fillna(0),
                    hist_df['velocity_x'].fillna(0)
                )
    
    agent_hist = hist_df[required_cols].fillna(0).values
    
    # Pad náº¿u thiáº¿u timesteps (pháº£i cÃ³ Ä‘á»§ 50 frames)
    if len(agent_hist) < 50:
        pad = np.zeros((50 - len(agent_hist), 5))
        agent_hist = np.vstack([pad, agent_hist])
    elif len(agent_hist) > 50:
        agent_hist = agent_hist[-50:]  # Láº¥y 50 frames cuá»‘i
    
    # ===== 3. Extract ground truth future (timesteps 50-109) =====
    future_df = focal_df[focal_df['timestep'] >= 50].copy()
    
    if len(future_df) == 0:
        gt_future = np.zeros((60, 2))
    else:
        gt_future = future_df[['position_x', 'position_y']].values
        
        # Pad náº¿u thiáº¿u
        if len(gt_future) < 60:
            last_pos = gt_future[-1] if len(gt_future) > 0 else np.zeros(2)
            pad = np.tile(last_pos, (60 - len(gt_future), 1))
            gt_future = np.vstack([gt_future, pad])
        elif len(gt_future) > 60:
            gt_future = gt_future[:60]
    
    return agent_hist.astype(np.float32), gt_future.astype(np.float32)

# %%
# Test processing
if scenarios:
    sample_df = load_scenario(scenarios[0])
    agent_hist, gt_future = process_av2_scenario(sample_df)
    print(f"âœ… Agent history shape: {agent_hist.shape}")  # Expected: (50, 5)
    print(f"âœ… Ground truth future shape: {gt_future.shape}")  # Expected: (60, 2)
    print(f"\nğŸ“ First history point: {agent_hist[0]}")
    print(f"ğŸ“ Last history point: {agent_hist[-1]}")
    print(f"ğŸ“ First future point: {gt_future[0]}")

# %% [markdown]
# ## 8. Load QCNet Model

# %%
# Import QCNet
try:
    from predictors import QCNet
    print("âœ… QCNet imported successfully")
except ImportError as e:
    print(f"âŒ Error importing QCNet: {e}")
    print("Trying alternative import...")
    from predictors.qcnet import QCNet

# %%
# Load model tá»« checkpoint
if os.path.exists(CKPT_PATH):
    model = QCNet.load_from_checkpoint(CKPT_PATH)
    model.eval()
    model = model.to(device)
    print(f"âœ… Model loaded from {CKPT_PATH}")
else:
    print(f"âŒ Checkpoint not found: {CKPT_PATH}")
    print("Downloading from HuggingFace...")
    !wget -q https://huggingface.co/ZikangZhou/QCNet/resolve/main/QCNet_AV2.ckpt -O qcnet_av2.ckpt
    model = QCNet.load_from_checkpoint('qcnet_av2.ckpt')
    model.eval()
    model = model.to(device)
    print("âœ… Model loaded from HuggingFace")

# %% [markdown]
# ## 9. Evaluation Metrics

# %%
from scipy.stats import gaussian_kde

def compute_ade(pred_trajs, gt_traj):
    """
    Average Displacement Error.
    pred_trajs: (num_modes, num_steps, 2)
    gt_traj: (num_steps, 2)
    """
    # Broadcast gt to match pred shape
    errors = np.linalg.norm(pred_trajs - gt_traj[None, :, :], axis=-1)  # (num_modes, num_steps)
    ade_per_mode = errors.mean(axis=1)  # (num_modes,)
    return ade_per_mode.min()  # minADE

def compute_fde(pred_trajs, gt_traj):
    """
    Final Displacement Error.
    """
    final_errors = np.linalg.norm(pred_trajs[:, -1, :] - gt_traj[-1, :], axis=-1)  # (num_modes,)
    return final_errors.min()  # minFDE

def compute_mr(pred_trajs, gt_traj, threshold=2.0):
    """
    Miss Rate - tá»· lá»‡ predictions cÃ³ FDE > threshold.
    """
    final_errors = np.linalg.norm(pred_trajs[:, -1, :] - gt_traj[-1, :], axis=-1)
    return float(final_errors.min() > threshold)

def compute_nll_kde(pred_trajs, gt_traj, bandwidth=0.1):
    """
    Negative Log-Likelihood via Kernel Density Estimation.
    Äo uncertainty cá»§a predictions.
    """
    num_modes, num_steps, _ = pred_trajs.shape
    num_gt_steps = len(gt_traj)
    
    nll_total = 0.0
    valid_steps = 0
    
    for t in range(min(num_steps, num_gt_steps)):
        pred_points = pred_trajs[:, t, :].T  # (2, num_modes)
        gt_point = gt_traj[t]
        
        # Skip náº¿u predictions quÃ¡ gáº§n nhau (KDE sáº½ fail)
        if np.std(pred_points) < 1e-6:
            continue
        
        try:
            kde = gaussian_kde(pred_points, bw_method=bandwidth)
            likelihood = kde(gt_point)[0]
            likelihood = max(likelihood, 1e-10)  # Avoid log(0)
            nll_total += -np.log2(likelihood)
            valid_steps += 1
        except:
            continue
    
    return nll_total / max(valid_steps, 1)

# %% [markdown]
# ## 10. Run Evaluation - Verify & Apply Patches
# 
# **ğŸ“ Note:** Náº¿u dÃ¹ng TrafficGamer repo (Ä‘Ã£ patched), section nÃ y chá»‰ verify.
# Náº¿u dÃ¹ng original QCNet, section nÃ y sáº½ apply cÃ¡c patches cáº§n thiáº¿t:
# 1. `TargetBuilder`: ThÃªm `forward()` method
# 2. `ArgoverseV2Dataset`: Disable auto-download cho Kaggle read-only filesystem

# %%
# ===== COMPREHENSIVE PATCHES =====
import oVERIFY & APPLY PATCHES =====
import os
import re
from pathlib import Path

print("ğŸ”§ Verifying and applying patches (if needed)
# ===== PATCH 1: TargetBuilder - Add forward() method =====
target_builder_file = Path(os.getcwd()) / 'transforms' / 'target_builder.py'

if target_builder_file.exists():
    print(f"1ï¸âƒ£ Patching TargetBuilder...")
    content = target_builder_file.read_text()
    
    if 'def forward(self' not in content:
        # Add forward method before __call__
        patch = '''    def forward(self, data):
        """Pass-through implementation for abstract method."""
        return data

    '''
        if 'def __call__(self' in content:
            content = content.replace('def __call__(self', patch + 'def __call__(self')
            target_builder_file.write_text(content)
            print("   âœ… Added forward() method to TargetBuilder")
        else:
            print("   âš ï¸ Could not find __call__ method")
    else:
        print("   âœ… TargetBuilder already patched")
else:
    print(f"   âŒ Not found: {target_builder_file}")

# ===== PATCH 2: ArgoverseV2Dataset - Disable auto-download =====
dataset_file = Path(os.getcwd()) / 'datasets' / 'argoverse_v2_dataset.py'

if dataset_file.exists():
    print(f"\n2ï¸âƒ£ Patching ArgoverseV2Dataset...")
    content = dataset_file.read_text()
    
    if '# KAGGLE_PATCHED' not in content:
        # Method 1: Override _download() method
        if 'def _download(self):' in content:
            old_download = '''    def _download(self):
        if files_exist(self.processed_paths):
            return
        self.download()'''
            
            new_download = '''    def _download(self):  # KAGGLE_PATCHED
        # Skip auto-download on Kaggle (read-only input)
        if not files_exist(self.processed_paths):
            print(f"âš ï¸ Processed files not found: {self.processed_paths}")
            print(f"âš ï¸ This is normal on Kaggle - using raw parquet files directly")
        return  # Skip download'''
            
            content = content.replace(old_download, new_download)
        
        # Method 2: Override download() method as well
        if 'def download(self):' in content:
            # Find the download method and replace it
            pattern = r'(    def download\(self\):.*?)(?=\n    def |\nclass |\Z)'
            replacement = '''    def download(self):  # KAGGLE_PATCHED
        """Disabled for Kaggle - data should be in input directory"""
        print("âš ï¸ Skipping download (Kaggle read-only filesystem)")
        return'''
            
            content = re.sub(pattern, replacement, content, flags=re.DOTALL)
        
        dataset_file.write_text(content)
        print("   âœ… Disabled auto-download in ArgoverseV2Dataset")
    else:
        print("   âœ… ArgoverseV2Dataset already patched")
else:
    print(f"   âŒ Not found: {dataset_file}")

# ===== PATCH 3: Use raw data directly (skip processed files requirement) =====
print(f"\n3ï¸âƒ£ Checking data structure...")

# Point val.py to the actual raw data location
input_data_path = Path(AV2_ROOT)

# Find actual scenario files
possible_splits = ['train', 'val', 'test']
found_data = False

for split in possible_splits:
    # Try different nesting levels
    for nested_path in [
        input_data_path / split / split,  # Kaggle: train/train/
        input_data_path / split,           # Standard: train/
        input_data_path,                   # Root
    ]:
        if nested_path.exists():
            scenarios = list(nested_path.glob("*/scenario_*.parquet"))
            if scenarios:
                print(f"   âœ… Found {len(scenarios)} scenarios in: {nested_path}")
                print(f"   ğŸ“ Split: {split}")
                found_data = True
                
                # Set this as the root for validation
                ACTUAL_DATA_ROOT = str(nested_path.parent)
                ACTUAL_SPLIT = split
                break
    if found_data:
        break

if not found_data:
    print("   âš ï¸ No scenario parquet files found!")
    print("   Listing directory structure:")
    for item in list(input_data_path.iterdir())[:5]:
        print(f"      - {item}")
        if item.is_dir():
            for sub in list(item.iterdir())[:3]:
                print(f"         - {sub}")
    ACTUAL_DATA_ROOT = AV2_ROOT
    ACTUAL_SPLIT = "train"

print(f"\nğŸ“Š Final configuration:")
print(f"   Data root: {ACTUAL_DATA_ROOT}")
print(f"   Split: {ACTUAL_SPLIT}")

# %%
# Run validation with actual data location
print(f"\n{'='*60}")
print(f"ğŸš€ Running QCNet Validation")
print(f"{'='*60}\n"TrafficGamer/QCNet Validation")
print(f"{'='*60}\n")

!python val.py --model QCNet --root {ACTUAL_DATA_ROOT} --ckpt_path {CKPT_PATH}

print(f"\n{'='*60}")
print("Expected metrics:")
print("  minFDE(K=6): ~1.25")
print("  minADE(K=6): ~0.72")
print("  MR(K=6): ~0.16")
print(f"
# Expected output:
# minFDE(K=6): ~1.25
# minADE(K=6): ~0.72  
# MR(K=6): ~0.16

# %% [markdown]
# ## 11. Alternative Evaluation Methods
# 
# **Note:** DataModule approach cÃ³ thá»ƒ gáº·p lá»—i `TargetBuilder` abstract class.  
# Náº¿u Section 10 khÃ´ng hoáº¡t Ä‘á»™ng, dÃ¹ng manual evaluation á»Ÿ Section 12 bÃªn dÆ°á»›i.

# %%
# âš ï¸ DataModule approach (cÃ³ thá»ƒ gáº·p lá»—i TargetBuilder)
# Uncomment náº¿u muá»‘n thá»­:

# from datamodules import ArgoverseV2DataModule

# datamodule = ArgoverseV2DataModule(
#     root=AV2_ROOT,
#     train_batch_size=1,
#     val_batch_size=1,
#     test_batch_size=1,
#     num_workers=0,
# )

# datamodule.setup('validate')
# val_loader = datamodule.val_dataloader()
# print(f"âœ… Validation loader ready: {len(val_loader)} batches")

# %% [markdown]
# ## 12. Manual Evaluation (Backup Method)
# 
# DÃ¹ng cÃ¡ch nÃ y náº¿u val.py script khÃ´ng hoáº¡t Ä‘á»™ng.  
# **âš ï¸ Warning:** Manual evaluation khÃ´ng bao gá»“m map data â†’ káº¿t quáº£ khÃ´ng chÃ­nh xÃ¡c!

# %%
def run_manual_evaluation(model, scenarios, device, num_samples=20):
    """
    Test metrics vá»›i dummy predictions.
    
    âš ï¸ Chá»‰ Ä‘á»ƒ test metrics functions, KHÃ”NG pháº£i official evaluation!
    QCNet cáº§n full data (agent + map + neighbors) Ä‘á»ƒ predict chÃ­nh xÃ¡c.
    """
    results = {
        'minADE': [],
        'minFDE': [],
        'MR': [],
    }
    
    print(f"ğŸ”„ Testing metrics vá»›i {num_samples} scenarios...")
    for scenario_path in tqdm(scenarios[:num_samples]):
        try:
            df = load_scenario(scenario_path)
            agent_hist, gt_future = process_av2_scenario(df)
            
            # Dummy predictions xung quanh GT
            pred_np = np.zeros((6, 60, 2))
            for mode_idx in range(6):
                noise = np.random.randn(60, 2) * 1.0
                offset = np.random.randn(2) * 3.0
                pred_np[mode_idx] = gt_future + noise + offset
            
            results['minADE'].append(compute_ade(pred_np, gt_future))
            results['minFDE'].append(compute_fde(pred_np, gt_future))
            results['MR'].append(compute_mr(pred_np, gt_future))
            
        except Exception as e:
            continue
    
    return results

# %%
# Run manual evaluation (chá»‰ Ä‘á»ƒ test)
if scenarios:
    print("\\nâš ï¸ Running MANUAL evaluation (dummy predictions - testing only)")
    print("âš ï¸ For official metrics, use Section 10: val.py script\\n")
    
    manual_results = run_manual_evaluation(
        model=model,
        scenarios=scenarios,
        device=device,
        num_samples=20
    )
    
    print("\\n" + "="*50)
    print("ğŸ“Š MANUAL TEST RESULTS (Not Real Predictions)")
    print("="*50)
    print(f"minADE: {np.mean(manual_results['minADE']):.3f} Â± {np.std(manual_results['minADE']):.3f}")
    print(f"minFDE: {np.mean(manual_results['minFDE']):.3f} Â± {np.std(manual_results['minFDE']):.3f}")
    print(f"MR:     {np.mean(manual_results['MR']):.3f} Â± {np.std(manual_results['MR']):.3f}")
    print("="*50)
    print("\\nâš ï¸ Use val.py script for real evaluation!")

# %% [markdown]
# ## 13. Visualization

# %%
import matplotlib.pyplot as plt

def visualize_prediction(agent_hist, gt_future, pred_trajs, pred_probs=None):
    """Visualize history, ground truth, vÃ  predictions."""
    fig, ax = plt.subplots(1, 1, figsize=(10, 10))
    
    # History
    hist_xy = agent_hist[:, :2]
    ax.plot(hist_xy[:, 0], hist_xy[:, 1], 'b-', linewidth=2, label='History')
    ax.scatter(hist_xy[-1, 0], hist_xy[-1, 1], c='blue', s=100, zorder=5, marker='o')
    
    # Ground truth future
    ax.plot(gt_future[:, 0], gt_future[:, 1], 'g-', linewidth=2, label='Ground Truth')
    ax.scatter(gt_future[-1, 0], gt_future[-1, 1], c='green', s=100, zorder=5, marker='*')
    
    # Predictions
    colors = plt.cm.Reds(np.linspace(0.3, 1, len(pred_trajs)))
    for i, pred in enumerate(pred_trajs):
        prob_str = f" ({pred_probs[i]:.2f})" if pred_probs is not None else ""
        ax.plot(pred[:, 0], pred[:, 1], '-', color=colors[i], 
                linewidth=1.5, alpha=0.7, label=f'Pred {i+1}{prob_str}')
    
    ax.set_xlabel('X (m)')
    ax.set_ylabel('Y (m)')
    ax.legend(loc='best')
    ax.set_title('Motion Prediction')
    ax.axis('equal')
    ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    return fig

# %%
# Visualize má»™t sample vá»›i real predictions (náº¿u cÃ³ thá»ƒ)
if scenarios:
    sample_df = load_scenario(scenarios[0])
    agent_hist, gt_future = process_av2_scenario(sample_df)
    
    # Try to get real predictions tá»« model
    try:
        # âš ï¸ LÆ°u Ã½: QCNet cáº§n full batch data (map, neighbors, etc.)
        # ÄÃ¢y lÃ  simplified version nÃªn cÃ³ thá»ƒ khÃ´ng chÃ­nh xÃ¡c
        
        # Prepare input (simplified - thiáº¿u map data)
        from datasets import ArgoverseV2Dataset
        
        # Load scenario vá»›i proper format
        scenario_id = scenarios[0].parent.name
        
        # Fallback: DÃ¹ng dummy predictions Ä‘á»ƒ visualize
        print("âš ï¸ Using dummy predictions for visualization")
        print("âš ï¸ For real predictions, run proper inference with full data")
        
        pred_trajs = np.zeros((6, 60, 2))
        for i in range(6):
            noise = np.random.randn(60, 2) * 0.5
            offset = np.random.randn(2) * 2.0
            pred_trajs[i] = gt_future + noise + offset
        
    except Exception as e:
        print(f"âš ï¸ Cannot run model inference: {e}")
        print("âš ï¸ Using dummy predictions for visualization demo")
        
        pred_trajs = np.zeros((6, 60, 2))
        for i in range(6):
            noise = np.random.randn(60, 2) * 0.5
            offset = np.random.randn(2) * 2.0
            pred_trajs[i] = gt_future + noise + offset
    
    visualize_prediction(agent_hist, gt_future, pred_trajs)

# %% [markdown]
# ## 14. Save Results

# %%
# Save evaluation results
import json

if 'manual_results' in locals():
    save_results = {
        'method': 'manual_test',
        'warning': 'Dummy predictions for testing only - NOT real model output',
        'minADE': float(np.mean(manual_results.get('minADE', [0]))),
        'minFDE': float(np.mean(manual_results.get('minFDE', [0]))),
        'MR': float(np.mean(manual_results.get('MR', [0]))),
        'num_samples': len(manual_results.get('minADE', [])),
    }
    
    with open('evaluation_results.json', 'w') as f:
        json.dump(save_results, f, indent=2)
    
    print("âœ… Results saved to evaluation_results.json")
    print(json.dumps(save_results, indent=2))
else:
    print("âš ï¸ No results to save. Run evaluation first!")
    print("   Use Section 10 (val.py) for official evaluation.")

# %% [markdown]
# ## ğŸ“ Notes
# 
# ### Argoverse 2 Data Format:
# - Parquet files chá»©a trajectory data
# - 110 timesteps (50 history + 60 future) @ 10Hz
# - Columns: track_id, object_type, object_category, timestep, position_x, position_y, heading, velocity_x, velocity_y
# 
# ### QCNet Expected Input:
# - QCNet cáº§n Ä‘áº§y Ä‘á»§ features: agent history, map data, neighbor agents
# - DÃ¹ng `ArgoverseV2DataModule` Ä‘á»ƒ load data Ä‘Ãºng format
# - Hoáº·c xem `datasets/argoverse_v2_dataset.py` Ä‘á»ƒ biáº¿t cÃ¡ch process
# 
# ### Troubleshooting:
# 1. **Checkpoint error**: Download láº¡i tá»« HuggingFace
# 2. **Data not found**: Check path structure, dÃ¹ng `glob` Ä‘á»ƒ tÃ¬m files
# 3. **CUDA OOM**: Giáº£m batch size hoáº·c dÃ¹ng CPU
# 4. **Import error**: Check PyG dependencies (torch-scatter, etc.)

# %% [markdown]
# ## Done! ğŸ‰
