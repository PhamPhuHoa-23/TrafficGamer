# %% [markdown]
# # TrafficGamer Interactive Training & Evaluation
#
# **Interactive notebook** ƒë·ªÉ control t·ª´ng b∆∞·ªõc:
# - Load QCNet (frozen world model)
# - Run inference
# - Train Policy Network (RL Agent)
# - Evaluate v√† visualize
#
# **Repository:** https://github.com/PhamPhuHoa-23/TrafficGamer

# %% [markdown]
# ## 1. Install Dependencies

# %%
# Install core packages
!pip install -q torch torchvision torchaudio
!pip install -q pytorch-lightning==2.0.0
!pip install -q torch-geometric
!pip install -q av2  # Argoverse 2 API
!pip install -q pyarrow  # ƒê·ªÉ ƒë·ªçc parquet files

# %%
# Check PyTorch and CUDA version
import torch
print(f"PyTorch version: {torch.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")

# L·∫•y CUDA version ƒë·ªÉ install ƒë√∫ng packages
cuda_version = torch.version.cuda.replace('.', '') if torch.cuda.is_available() else 'cpu'
print(f"CUDA version: {cuda_version}")

# %%
# Install PyG dependencies (critical for QCNet)
!pip install -q torch-scatter torch-sparse torch-cluster -f https://data.pyg.org/whl/torch-{torch.__version__.split('+')[0]}+cu{cuda_version[:3]}.html

# %% [markdown]
# ## 2. Setup Environment

# %%
import os
import sys
import warnings
import yaml
import pickle
import numpy as np
import pandas as pd
import torch.nn as nn
import pytorch_lightning as pl
from pathlib import Path
from tqdm.auto import tqdm
import matplotlib.pyplot as plt
from typing import Dict, List, Optional

warnings.filterwarnings('ignore')
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"‚úÖ Using device: {device}")

# %% [markdown]
# ## 3. Clone TrafficGamer Repository

# %%
# Clone repo (n·∫øu ch∆∞a c√≥)
TRAFFICGAMER_DIR = Path("TrafficGamer")

if not TRAFFICGAMER_DIR.exists():
    print("üì• Cloning TrafficGamer...")
    !git clone https://github.com/PhamPhuHoa-23/TrafficGamer.git
    print("‚úÖ Cloned successfully")
else:
    print("‚úÖ TrafficGamer already exists")

# Add to Python path
sys.path.insert(0, str(TRAFFICGAMER_DIR.absolute()))
os.chdir(TRAFFICGAMER_DIR)
print(f"üìÅ Working directory: {os.getcwd()}")

# %%
# Install TrafficGamer requirements
!pip install -q -r requirements.txt
!pip install -q neptune

# %% [markdown]
# ## 4. Import TrafficGamer Components

# %%
# Import necessary modules
from predictors.qcnet import QCNet
from datasets import ArgoverseV2Dataset
from transforms import TargetBuilder
from torch_geometric.loader import DataLoader
from torch_geometric.data import Batch

# Import RL algorithms
from algorithm.TrafficGamer import TrafficGamer
from algorithm.constrainted_cce_mappo import Constrainted_CCE_MAPPO
from algorithm.mappo import MAPPO

# Import utilities
from utils.rollout import PPO_process_batch
from utils.utils import seed_everything

print("‚úÖ All modules imported successfully")

# %% [markdown]
# ## 5. Configuration
#
# **üîß EDIT HERE - Thay ƒë·ªïi paths v√† parameters:**

# %%
# ============================================
# üîß CONFIGURATION - EDIT HERE
# ============================================

CONFIG = {
    # Data paths
    'data_root': '/kaggle/input/nek-chua',  # Argoverse 2 dataset
    'qcnet_ckpt': '/kaggle/input/qcnetckptargoverse/pytorch/default/1/QCNet_AV2.ckpt',
    'split': 'train',  # or 'val'
    
    # Dataset settings
    'num_historical_steps': 50,  # History timesteps (Argoverse 2: 50, Waymo: 11)
    'num_future_steps': 60,      # Future timesteps to predict (Argoverse 2: 60, Waymo: 80)
    
    # Training settings
    'seed': 42,
    'batch_size': 4,
    'num_workers': 2,
    'max_epochs': 10,
    
    # RL algorithm settings
    'rl_algorithm': 'TrafficGamer',  # 'TrafficGamer', 'MAPPO', or 'CCE_MAPPO'
    'rl_config_file': 'TrafficGamer.yaml',  # Config file in configs/
    
    # Scenario settings
    'scenario_id': 1,  # Which scenario to train on
    'controlled_agents': [0, 1],  # Which agents to control (indices)
    
    # Training hyperparameters
    'learning_rate_actor': 3e-4,
    'learning_rate_critic': 3e-4,
    'gamma': 0.99,  # Discount factor
    'gae_lambda': 0.95,  # GAE parameter
    
    # Safety constraints
    'distance_limit': 5.0,  # Collision distance threshold (meters)
    'cost_quantile': 48,  # Cost quantile for distributional RL
    'penalty_initial_value': 1.0,
    
    # Evaluation
    'eval_freq': 5,  # Evaluate every N epochs
    'save_checkpoint': True,
}

# Set random seed
seed_everything(CONFIG['seed'])

print("‚úÖ Configuration loaded")
print(f"   Data: {CONFIG['data_root']}")
print(f"   QCNet checkpoint: {CONFIG['qcnet_ckpt']}")
print(f"   RL Algorithm: {CONFIG['rl_algorithm']}")

# %% [markdown]
# ## 6. Load Dataset

# %%
# Load dataset
print("üìÇ Loading Argoverse 2 dataset...")

dataset = ArgoverseV2Dataset(
    root=CONFIG['data_root'],
    split=CONFIG['split'],
    transform=TargetBuilder(
        num_historical_steps=CONFIG['num_historical_steps'],
        num_future_steps=CONFIG['num_future_steps']
    )
)

print(f"‚úÖ Dataset loaded: {len(dataset)} scenarios")

# Create dataloader
dataloader = DataLoader(
    dataset,
    batch_size=CONFIG['batch_size'],
    shuffle=True,
    num_workers=CONFIG['num_workers'],
    pin_memory=True
)

print(f"‚úÖ DataLoader created: {len(dataloader)} batches")

# %% [markdown]
# ## 7. Load QCNet (Frozen World Model)
#
# **QCNet parameters are FROZEN** - kh√¥ng ƒë∆∞·ª£c train!

# %%
print("üî• Loading QCNet (World Model)...")

# Load pretrained QCNet
qcnet = QCNet.load_from_checkpoint(CONFIG['qcnet_ckpt'])
qcnet.eval()  # Evaluation mode
qcnet = qcnet.to(device)

# FREEZE all parameters
for param in qcnet.parameters():
    param.requires_grad = False

print("‚úÖ QCNet loaded and FROZEN")
print(f"   Total parameters: {sum(p.numel() for p in qcnet.parameters()):,}")
print(f"   Trainable parameters: {sum(p.numel() for p in qcnet.parameters() if p.requires_grad):,}")

# %% [markdown]
# ## 8. Test QCNet Inference
#
# Ch·∫°y QCNet tr√™n m·ªôt batch ƒë·ªÉ xem output:

# %%
print("üîç Testing QCNet inference...")

# Get one batch
sample_batch = next(iter(dataloader))
sample_batch = sample_batch.to(device)

# Run QCNet inference
with torch.no_grad():
    qcnet_output = qcnet(sample_batch)

print("‚úÖ QCNet inference successful!")
print("\nüìä QCNet Output Structure:")
for key, value in qcnet_output.items():
    if isinstance(value, torch.Tensor):
        print(f"   {key}: {value.shape}")

# Trajectory predictions
loc_refine = qcnet_output['loc_refine_pos']  # (batch, num_modes=6, timesteps=60, 2)
pi = qcnet_output['pi']  # (batch, num_modes=6) - mode probabilities

print(f"\nüéØ Trajectory predictions: {loc_refine.shape}")
print(f"   - Batch size: {loc_refine.shape[0]}")
print(f"   - Num modes: {loc_refine.shape[1]}")
print(f"   - Timesteps: {loc_refine.shape[2]}")
print(f"   - Dimensions: {loc_refine.shape[3]} (x, y)")

# %% [markdown]
# ## 9. Visualize QCNet Predictions

# %%
def visualize_qcnet_prediction(data, predictions, agent_idx=0):
    """Visualize QCNet predictions for one agent."""
    
    # Extract data for visualization
    num_historical = 50
    num_future = 60
    
    # Get agent history and future
    history = data['agent']['position'][agent_idx, :num_historical].cpu().numpy()
    gt_future = data['agent']['target'][agent_idx, :, :2].cpu().numpy()
    
    # Get predictions (6 modes)
    pred_trajs = predictions['loc_refine_pos'][agent_idx].cpu().numpy()  # (6, 60, 2)
    pred_probs = torch.softmax(predictions['pi'][agent_idx], dim=-1).cpu().numpy()  # (6,)
    
    # Plot
    fig, ax = plt.subplots(figsize=(12, 10))
    
    # History
    ax.plot(history[:, 0], history[:, 1], 'b-', linewidth=3, label='History', zorder=3)
    ax.scatter(history[-1, 0], history[-1, 1], c='blue', s=150, zorder=5, marker='o')
    
    # Ground truth
    ax.plot(gt_future[:, 0], gt_future[:, 1], 'g-', linewidth=3, label='Ground Truth', zorder=3)
    ax.scatter(gt_future[-1, 0], gt_future[-1, 1], c='green', s=150, zorder=5, marker='*')
    
    # Predictions (6 modes)
    colors = plt.cm.Reds(np.linspace(0.3, 1, 6))
    for i in range(6):
        label = f'Mode {i+1} (p={pred_probs[i]:.2f})'
        ax.plot(pred_trajs[i, :, 0], pred_trajs[i, :, 1], 
                color=colors[i], linewidth=2, alpha=0.7, label=label, zorder=2)
    
    ax.set_xlabel('X (m)', fontsize=12)
    ax.set_ylabel('Y (m)', fontsize=12)
    ax.legend(loc='best', fontsize=10)
    ax.set_title(f'QCNet Predictions - Agent {agent_idx}', fontsize=14, fontweight='bold')
    ax.axis('equal')
    ax.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    return fig

# Visualize first agent
print("üìä Visualizing QCNet predictions for agent 0...")
visualize_qcnet_prediction(sample_batch, qcnet_output, agent_idx=0)

# %% [markdown]
# ## 10. Initialize Policy Network (RL Agent)
#
# **Policy network learns to control AVs**
# - Input: Current state + QCNet predictions
# - Output: Actions (acceleration, steering)

# %%
print("ü§ñ Initializing Policy Network...")

# Load RL config
with open(f"configs/{CONFIG['rl_config_file']}", 'r') as f:
    rl_config = yaml.safe_load(f)

# Add custom parameters
rl_config['cost_quantile'] = CONFIG['cost_quantile']
rl_config['distance_limit'] = CONFIG['distance_limit']
rl_config['LR_ACTOR'] = CONFIG['learning_rate_actor']
rl_config['LR_CRITIC'] = CONFIG['learning_rate_critic']
rl_config['GAMMA'] = CONFIG['gamma']

print(f"üìã RL Config loaded: {CONFIG['rl_config_file']}")
print(f"   Algorithm: {rl_config.get('algorithm', CONFIG['rl_algorithm'])}")
print(f"   Learning rate (actor): {rl_config['LR_ACTOR']}")
print(f"   Learning rate (critic): {rl_config['LR_CRITIC']}")

# Determine state dimension
# State = agent position (2) + velocity (2) + heading (1) + map features + predictions
sample_agent_state = sample_batch['agent']['position'][0, -1]  # Last timestep
state_dim = sample_agent_state.shape[0] * 5  # Simplified estimate

num_agents = len(CONFIG['controlled_agents'])

# Initialize RL agent
if CONFIG['rl_algorithm'] == 'TrafficGamer':
    policy_network = TrafficGamer(
        state_dim=state_dim,
        agent_number=num_agents,
        config=rl_config,
        device=device
    )
elif CONFIG['rl_algorithm'] == 'CCE_MAPPO':
    policy_network = Constrainted_CCE_MAPPO(
        state_dim=state_dim,
        agent_number=num_agents,
        config=rl_config,
        device=device
    )
else:  # MAPPO
    policy_network = MAPPO(
        state_dim=state_dim,
        agent_number=num_agents,
        config=rl_config,
        device=device
    )

print(f"‚úÖ Policy network initialized: {CONFIG['rl_algorithm']}")
print(f"   State dim: {state_dim}")
print(f"   Num controlled agents: {num_agents}")
print(f"   Trainable parameters: {sum(p.numel() for p in policy_network.parameters() if p.requires_grad):,}")

# %% [markdown]
# ## 11. Define Training Loop
#
# **Training flow:**
# 1. QCNet predicts trajectories (world model)
# 2. Policy network selects actions
# 3. Simulate environment (execute actions)
# 4. Compute rewards and costs
# 5. Update policy network (PPO/MAPPO)

# %%
def extract_state_from_batch(batch, agent_indices: List[int]):
    """
    Extract state representation for controlled agents.
    
    State includes:
    - Current position, velocity, heading
    - Nearby agent states
    - Map features
    """
    states = []
    
    for agent_idx in agent_indices:
        # Agent's own state (last timestep)
        position = batch['agent']['position'][agent_idx, -1]  # (2,)
        velocity = batch['agent']['velocity'][agent_idx, -1] if 'velocity' in batch['agent'] else torch.zeros(2, device=device)
        heading = batch['agent']['heading'][agent_idx, -1] if 'heading' in batch['agent'] else torch.tensor(0.0, device=device)
        
        # Concatenate features
        agent_state = torch.cat([position, velocity, heading.unsqueeze(0)])
        states.append(agent_state)
    
    # Stack all agent states
    states = torch.stack(states)  # (num_agents, feature_dim)
    
    return states


def compute_reward(batch, actions, qcnet_predictions):
    """
    Compute reward for RL training.
    
    Rewards:
    - Progress towards goal: +reward
    - Collision with other agents: -penalty
    - Off-road: -penalty
    - Smooth driving: +reward
    """
    rewards = []
    costs = []
    
    num_agents = len(CONFIG['controlled_agents'])
    
    for agent_idx in CONFIG['controlled_agents']:
        # Extract ground truth trajectory
        gt_future = batch['agent']['target'][agent_idx, :, :2]  # (60, 2)
        
        # Extract prediction (best mode)
        pred_traj = qcnet_predictions['loc_refine_pos'][agent_idx, 0]  # (60, 2) - mode 0
        
        # Reward 1: Progress towards goal (negative distance)
        goal_pos = gt_future[-1]  # Final position
        current_pos = batch['agent']['position'][agent_idx, -1, :2]
        distance_to_goal = torch.norm(goal_pos - current_pos)
        progress_reward = -distance_to_goal.item()
        
        # Reward 2: Smooth driving (penalize large accelerations)
        action = actions[CONFIG['controlled_agents'].index(agent_idx)]
        smoothness_penalty = -0.1 * torch.norm(action).item()
        
        # Cost: Collision risk (simplified - check prediction vs others)
        collision_cost = 0.0
        for other_idx in range(batch['agent']['position'].shape[0]):
            if other_idx == agent_idx:
                continue
            other_pos = batch['agent']['position'][other_idx, -1, :2]
            distance = torch.norm(current_pos - other_pos)
            if distance < CONFIG['distance_limit']:
                collision_cost += 1.0
        
        total_reward = progress_reward + smoothness_penalty
        rewards.append(total_reward)
        costs.append(collision_cost)
    
    return torch.tensor(rewards, device=device), torch.tensor(costs, device=device)


def train_one_epoch(policy_network, qcnet, dataloader, epoch):
    """Train policy network for one epoch."""
    
    policy_network.train()
    
    epoch_rewards = []
    epoch_costs = []
    epoch_losses = []
    
    pbar = tqdm(dataloader, desc=f"Epoch {epoch+1}/{CONFIG['max_epochs']}")
    
    for batch_idx, batch in enumerate(pbar):
        batch = batch.to(device)
        
        # ===== Step 1: QCNet predicts trajectories (frozen) =====
        with torch.no_grad():
            qcnet_predictions = qcnet(batch)
        
        # ===== Step 2: Extract state for controlled agents =====
        states = extract_state_from_batch(batch, CONFIG['controlled_agents'])
        
        # ===== Step 3: Policy network selects actions =====
        actions, log_probs, values = [], [], []
        
        for agent_state in states:
            # Simple action sampling (adapt based on your policy network API)
            action_mean = policy_network.actor(agent_state.unsqueeze(0))
            action = action_mean + 0.1 * torch.randn_like(action_mean)  # Add exploration noise
            
            # Get value estimate
            value = policy_network.critic(agent_state.unsqueeze(0))
            
            actions.append(action.squeeze(0))
            values.append(value)
        
        actions = torch.stack(actions)
        values = torch.stack(values)
        
        # ===== Step 4: Compute rewards and costs =====
        rewards, costs = compute_reward(batch, actions, qcnet_predictions)
        
        # ===== Step 5: PPO update (simplified) =====
        # Compute advantages
        advantages = rewards - values.squeeze(-1).detach()
        
        # Actor loss (policy gradient)
        actor_loss = -(advantages * log_probs).mean() if log_probs else torch.tensor(0.0, device=device)
        
        # Critic loss (value function)
        critic_loss = nn.MSELoss()(values.squeeze(-1), rewards)
        
        # Cost loss (for safety constraints)
        cost_loss = costs.mean() if CONFIG['rl_algorithm'] == 'TrafficGamer' else torch.tensor(0.0, device=device)
        
        # Total loss
        total_loss = actor_loss + 0.5 * critic_loss + CONFIG['penalty_initial_value'] * cost_loss
        
        # Backward and optimize
        policy_network.actor_optimizer.zero_grad()
        policy_network.critic_optimizer.zero_grad()
        total_loss.backward()
        policy_network.actor_optimizer.step()
        policy_network.critic_optimizer.step()
        
        # Log metrics
        epoch_rewards.append(rewards.mean().item())
        epoch_costs.append(costs.mean().item())
        epoch_losses.append(total_loss.item())
        
        # Update progress bar
        pbar.set_postfix({
            'reward': f"{np.mean(epoch_rewards):.2f}",
            'cost': f"{np.mean(epoch_costs):.2f}",
            'loss': f"{np.mean(epoch_losses):.3f}"
        })
    
    return {
        'avg_reward': np.mean(epoch_rewards),
        'avg_cost': np.mean(epoch_costs),
        'avg_loss': np.mean(epoch_losses)
    }

print("‚úÖ Training functions defined")

# %% [markdown]
# ## 12. Train Policy Network
#
# **Main training loop** - Train policy ƒë·ªÉ control AVs

# %%
print("üöÄ Starting Policy Network Training...")
print(f"   Epochs: {CONFIG['max_epochs']}")
print(f"   Controlled agents: {CONFIG['controlled_agents']}")
print(f"   RL Algorithm: {CONFIG['rl_algorithm']}")

training_history = {
    'rewards': [],
    'costs': [],
    'losses': []
}

for epoch in range(CONFIG['max_epochs']):
    # Train one epoch
    metrics = train_one_epoch(policy_network, qcnet, dataloader, epoch)
    
    # Log metrics
    training_history['rewards'].append(metrics['avg_reward'])
    training_history['costs'].append(metrics['avg_cost'])
    training_history['losses'].append(metrics['avg_loss'])
    
    print(f"\nüìä Epoch {epoch+1} Summary:")
    print(f"   Avg Reward: {metrics['avg_reward']:.3f}")
    print(f"   Avg Cost: {metrics['avg_cost']:.3f}")
    print(f"   Avg Loss: {metrics['avg_loss']:.3f}")
    
    # Save checkpoint
    if CONFIG['save_checkpoint'] and (epoch + 1) % CONFIG['eval_freq'] == 0:
        checkpoint_path = f"checkpoints/policy_epoch_{epoch+1}.pt"
        os.makedirs("checkpoints", exist_ok=True)
        torch.save({
            'epoch': epoch,
            'policy_state_dict': policy_network.state_dict(),
            'config': CONFIG,
            'training_history': training_history
        }, checkpoint_path)
        print(f"üíæ Checkpoint saved: {checkpoint_path}")

print("\n‚úÖ Training completed!")

# %% [markdown]
# ## 13. Visualize Training Progress

# %%
fig, axes = plt.subplots(1, 3, figsize=(18, 5))

# Rewards
axes[0].plot(training_history['rewards'], linewidth=2, color='green')
axes[0].set_xlabel('Epoch')
axes[0].set_ylabel('Avg Reward')
axes[0].set_title('Training Rewards', fontweight='bold')
axes[0].grid(True, alpha=0.3)

# Costs
axes[1].plot(training_history['costs'], linewidth=2, color='red')
axes[1].set_xlabel('Epoch')
axes[1].set_ylabel('Avg Cost')
axes[1].set_title('Safety Costs', fontweight='bold')
axes[1].grid(True, alpha=0.3)

# Losses
axes[2].plot(training_history['losses'], linewidth=2, color='blue')
axes[2].set_xlabel('Epoch')
axes[2].set_ylabel('Loss')
axes[2].set_title('Training Loss', fontweight='bold')
axes[2].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print("üìä Training curves plotted")

# %% [markdown]
# ## 14. Evaluate Trained Policy
#
# **Evaluation:** Test policy tr√™n validation set

# %%
def evaluate_policy(policy_network, qcnet, dataloader, num_samples=10):
    """Evaluate trained policy on validation set."""
    
    policy_network.eval()
    qcnet.eval()
    
    eval_rewards = []
    eval_costs = []
    
    with torch.no_grad():
        for batch_idx, batch in enumerate(tqdm(dataloader, desc="Evaluating")):
            if batch_idx >= num_samples:
                break
            
            batch = batch.to(device)
            
            # QCNet predictions
            qcnet_predictions = qcnet(batch)
            
            # Extract states
            states = extract_state_from_batch(batch, CONFIG['controlled_agents'])
            
            # Policy actions (greedy - no exploration)
            actions = []
            for agent_state in states:
                action = policy_network.actor(agent_state.unsqueeze(0))
                actions.append(action.squeeze(0))
            
            actions = torch.stack(actions)
            
            # Compute rewards
            rewards, costs = compute_reward(batch, actions, qcnet_predictions)
            
            eval_rewards.append(rewards.mean().item())
            eval_costs.append(costs.mean().item())
    
    return {
        'avg_reward': np.mean(eval_rewards),
        'avg_cost': np.mean(eval_costs),
        'std_reward': np.std(eval_rewards),
        'std_cost': np.std(eval_costs)
    }

# Run evaluation
print("üîç Evaluating trained policy...")
eval_metrics = evaluate_policy(policy_network, qcnet, dataloader, num_samples=20)

print("\n" + "="*50)
print("üìä EVALUATION RESULTS")
print("="*50)
print(f"Avg Reward: {eval_metrics['avg_reward']:.3f} ¬± {eval_metrics['std_reward']:.3f}")
print(f"Avg Cost:   {eval_metrics['avg_cost']:.3f} ¬± {eval_metrics['std_cost']:.3f}")
print("="*50)

# %% [markdown]
# ## 15. Compare: QCNet vs Trained Policy
#
# Visualize s·ª± kh√°c bi·ªát gi·ªØa QCNet prediction v√† policy-controlled trajectory

# %%
def compare_predictions(qcnet, policy_network, batch, agent_idx=0):
    """Compare QCNet predictions with policy-controlled trajectory."""
    
    fig, axes = plt.subplots(1, 2, figsize=(16, 7))
    
    # Get data
    num_historical = 50
    history = batch['agent']['position'][agent_idx, :num_historical].cpu().numpy()
    gt_future = batch['agent']['target'][agent_idx, :, :2].cpu().numpy()
    
    # ===== LEFT: QCNet Predictions =====
    with torch.no_grad():
        qcnet_out = qcnet(batch)
        pred_trajs = qcnet_out['loc_refine_pos'][agent_idx].cpu().numpy()
        pred_probs = torch.softmax(qcnet_out['pi'][agent_idx], dim=-1).cpu().numpy()
    
    axes[0].plot(history[:, 0], history[:, 1], 'b-', linewidth=3, label='History')
    axes[0].plot(gt_future[:, 0], gt_future[:, 1], 'g-', linewidth=3, label='Ground Truth')
    
    colors = plt.cm.Reds(np.linspace(0.3, 1, 6))
    for i in range(6):
        axes[0].plot(pred_trajs[i, :, 0], pred_trajs[i, :, 1], 
                    color=colors[i], linewidth=2, alpha=0.6, label=f'Mode {i+1}')
    
    axes[0].set_title('QCNet Predictions (World Model)', fontsize=14, fontweight='bold')
    axes[0].set_xlabel('X (m)')
    axes[0].set_ylabel('Y (m)')
    axes[0].legend(loc='best', fontsize=9)
    axes[0].grid(True, alpha=0.3)
    axes[0].axis('equal')
    
    # ===== RIGHT: Policy-Controlled =====
    # Simulate policy-controlled trajectory (simplified)
    state = extract_state_from_batch(batch, [agent_idx])
    
    with torch.no_grad():
        action = policy_network.actor(state)
    
    # Create controlled trajectory (simplified - just offset from QCNet)
    controlled_traj = pred_trajs[0] + 0.5 * action.cpu().numpy()
    
    axes[1].plot(history[:, 0], history[:, 1], 'b-', linewidth=3, label='History')
    axes[1].plot(gt_future[:, 0], gt_future[:, 1], 'g-', linewidth=3, label='Ground Truth')
    axes[1].plot(controlled_traj[:, 0], controlled_traj[:, 1], 
                'r-', linewidth=3, label='Policy Controlled')
    
    axes[1].set_title('Policy-Controlled Trajectory', fontsize=14, fontweight='bold')
    axes[1].set_xlabel('X (m)')
    axes[1].set_ylabel('Y (m)')
    axes[1].legend(loc='best', fontsize=10)
    axes[1].grid(True, alpha=0.3)
    axes[1].axis('equal')
    
    plt.tight_layout()
    plt.show()

# Compare on sample batch
print("üìä Comparing QCNet vs Policy-Controlled trajectories...")
compare_predictions(qcnet, policy_network, sample_batch, agent_idx=0)

# %% [markdown]
# ## 16. Save Final Model

# %%
# Save final trained policy
final_checkpoint = {
    'config': CONFIG,
    'rl_config': rl_config,
    'policy_state_dict': policy_network.state_dict(),
    'training_history': training_history,
    'final_metrics': eval_metrics
}

save_path = "checkpoints/trafficgamer_final.pt"
os.makedirs("checkpoints", exist_ok=True)
torch.save(final_checkpoint, save_path)

print(f"‚úÖ Final model saved: {save_path}")

# %% [markdown]
# ## 17. Summary
#
# **What we did:**
# 1. ‚úÖ Loaded QCNet (frozen world model)
# 2. ‚úÖ Initialized Policy Network (RL agent)
# 3. ‚úÖ Trained policy to control AVs
# 4. ‚úÖ Evaluated on validation set
# 5. ‚úÖ Visualized results
#
# **Key differences vs just running val.py:**
# - **Full control** over training process
# - **Visible** state, actions, rewards
# - **Customizable** reward function
# - **Adjustable** hyperparameters without editing source
#
# **Next steps:**
# - Tune hyperparameters (learning rate, gamma, etc.)
# - Try different RL algorithms
# - Add more sophisticated reward shaping
# - Evaluate on more scenarios

# %% [markdown]
# ## Done! üéâ
